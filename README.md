# Streaming de Dados em Tempo Real

## Conceitos ‚úíÔ∏è
Tamb√©m conhecido como Evento de Processamento de Streaming, Streaming de Dados trata do fluxo cont√≠nuo de gera√ß√£o de dados √† partir de fontes diversas.
Utilizando este tipo de tecnologia, streaming de dados podem ser consumidos, processados, armazenados e analisados conforme estes s√£o gerados em tempo real.

Streaming √© o conceito usado para descrever um fluxo cont√≠nuo - sem in√≠cio ou fim - que prov√™ uma contante carga de dados que pode ser utilizada sem a necessidade pr√©via de download.
Streaming de Dados, de modo semelhante, s√£o gerados por v√°rias fontes e formatos distintos e com volumes vari√°veis, como por exemplo: aplica√ß√µes WEB/ERPs; dispositivos de rede; transa√ß√µes banc√°rias; e dados de localia√ß√£o (GPS). Todos estes dados poder ser agregados para an√°lise e gera√ß√£o de informa√ß√£o.  

### Como funciona?
"Dados modernos" s√£o gerados por uma infinidade de dispositivos distintos e, considerando estudos como Internet das Coisas, esta quest√£o torna-se exponencial. Deste modo, se torna praticamente imposs√≠vel a regulamenta√ß√£o ou padroniza√ß√£o de estrutura de dados, visando controle de volume e frequ√™ncia na gera√ß√£o dos dados. 

Aplica√ß√µes que analisam dados por streaming precisam process√°-los individualmente ou por conjunto - segundo regras propostas - e em ordem sequencial. Deixando impl√≠cita a relev√¢ncia destes sistemas serem preparados para toler√¢ncia √† falhas. Cada parcela de dados gerada trat√° sua fonte origem e timestamp, para habilitar aplica√ß√µes a trabalharem corretamento com o streaming.

#### Fun√ß√µes principais:
- Storage (armazenamento): deve ser capaz de gravar grandes conjuntos de streaming de dadosde modo sequencial e consistente;
- Processing (processamento): deve interagir com o storage, consumir, analisar e rodar l√≥gicas computacionais em cima dos dados.

### Principais desafios:
Para o usu√°rio final, o streaming de dados em tempo real precisa ter tanta qualidade quanto qualquer sistema rodando em batch, com dados j√° previamente armazenados em disco, cache ou DB. Para tanto, alguns aspectos da aplica√ß√£o devem ser reconhecidos como obrigat√≥rios.
- Escalabilidade

    - Um exemplo muito pr√°tico, √© que √† medida que falhas acontecem, a quantidade de dados de log podem passar de kilobits para gigabits/seg. Esta exponencialidade exige que a infraestrutura como um todo cres√ßa e se adapte instat√¢neamente para que n√£o hava nenhum sentido de perda de dados. 

- Ordena√ß√£o

    - Parte n√£o trivial para streaming em tempo real, principalmente porqu√™ nem sempre a ordem dos dados que √© enviada de uma fonte √© a mesma ordem que chega ao destino.

- Consist√™ncia e durabilidade

    - Tamb√©m um dos grandes desafios, considerando que qualquer dado gerado √† qualquer hora ou volume, pode ser modificado, transferido para outro DC e enviado ao destino em outra parte do mundo.

- Toler√¢ncia √† falhas e garantias de dados

    - Com dados vindos de in√∫meras fontes, localiza√ß√µes, formatos e volumes, o sistema realmente est√° preparado para prevenir perturba√ß√µes de um √∫nico ponto de falha? 

## KAFKA CLI üîì
O Apache Kafka veio como uma resposta inteligente e segura aos principais desafios do streaming de dados. 
√â um sistema de mensagems distribu√≠dos, com alta vaz√£o e capaz de gerar, analisar e monitorar dados em tempo real. 

Pode ser visto como um sistema de publisher qsubscriber, como o Youtube, mas com mensagens, n√£o v√≠deos.

> <i>Em resumo, temos os t√≥picos (streaming espec√≠fico); as mensagens (associadas a t√≥picos); producers (geram dados); os consumers (se inscrevem em um ou mais t√≥picos para consumirem as mensagens); e o Kafka em si (broker que funciona junto ao Zookeeper para gerenciar os t√≥picos)</i>

### Setup do Kafka
#### 1. Download
```bash
curl http://ftp.unicamp.br/pub/apache/kafka/2.7.0/kafka_2.12-2.7.0.tgz -o ~/Downloads/kafka.tgz
```

#### 2. Criar projeto 
```bash
mkdir kafka
cd kafka
tar -xvzf ~/Downloads/kafka.tgz --strip 1
```

#### 3. Abrir projeto ([VS Code](https://code.visualstudio.com/download))
```bash
code .
```

#### 4. Iniciando Zookeper
```bash
bin/zookeeper-server-start.sh config/zookeeper.properties
```

#### 5. Iniciando Kafka
```bash
bin/kafka-server-start.sh config/server.properties
```

### Exemplo
Utilizando os comandos (CLI) do Apache KAFKA, vamos criar o seguinte esquema de troca de mensagens via streaming de dados

<img src="https://raw.githubusercontent.com/robertomorel/real-time-streaming-data-processing/master/assets/kafka-cli.jpeg" alt="Esquema, Roberto's header" width="500"/>

#### Criando t√≥picos
- petr4: `bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --topic petr4`
- vale3: `bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --topic vale3`

> Para listar os t√≥picos existentes: `bin/kafka-topics.sh --list --zookeeper localhost:2181`

#### Criando produtores
- Produtor 1: `bin/kafka-console-producer.sh --broker-list localhost:9092 --topic petr4`
- Produtor 2: `bin/kafka-console-producer.sh --broker-list localhost:9092 --topic vale3`

#### Criando consumidores
- Consumidor 1: `bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic vale3 --from-beginning`
- Consumidor 2: `bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic vale3 --from-beginning`
- Consumidor 3: `bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic petr4 --from-beginning`

<hr>
<b>
  <p align="center">
    RESULTADO ESPERADO
  </p>
</b>

<div align="center">
  <img src="https://github.com/robertomorel/real-time-streaming-data-processing/blob/master/assets/kafka-zookeeper.gif" width="700"/>
</div>

## Projetos üì•

### [Produtor Consumidor](/produtor-consumidor)
Regras:
- No consumidor (classe Consumer) imprima apenas as tuplas que possuem valores de IMC acima de 35 (IMC = peso/altura¬≤);
- Crie um novo produtor (classe Producer2, por exemplo) o qual gera um streaming contendo ‚Äúnomes‚Äù e ‚Äúsal√°rios‚Äù aleat√≥rios no intervalo fixo de 4 segundos. Os nomes aleat√≥rios podem ser gerados pela biblioteca Faker utilizada no curso e os sal√°rios devem estar no intervalo entre R$ 1.000,00 e R$ 3.000,00;
- Crie um novo consumidor (classe Consumer2, por exemplo) o qual ir√° consumir os dados do novo produtor criado e imprimir o valor de cada tupla;
- Altere o consumidor rec√©m criado para imprimir apenas o nome das pessoas que recebem sal√°rios maiores que R$ 2.000,00;
- Aumente a frequ√™ncia de gera√ß√£o das tuplas para 2 segundos no Produtor 1 (classe Producer);
- Gere dados simult√¢neos de dois Produtores da classe Producer e dois produtores da classe Producer2.

#### Para executar
- Inicie o Zookeeper e o Kafka;
- Entre na pasta do projeto;
- Rode o comando:

  ```bash
  python3 producer_consumer.py
  ```

### [Produtor Consumidor - MongoDB](/produtor-consumidor-mongodb)
Regras:
- Inclua na gera√ß√£o de tuplas o nome das pessoas como primeiro atributo. A estrutura do seu arquivo json gerado via streaming ser√°: nome, idade, altura e peso. O nome pode ser gerado pela biblioteca Faker;
- Pesquise como seria para salvar os dados em um banco de dados MySQL ou algum outro banco de dados relacional de sua prefer√™ncia. Salve nesse banco (via consumidor) apenas as pessoas com nomes que come√ßam com a letra J.

#### Depend√™ncia
Necess√°rio o MySQL na m√°quina, deste modo, recomendamos o uso do docker.

Clique [aqui](https://hub.docker.com/_/mysql) para realizar o setup do MySQL a partir de um container do DockerHub.

#### Para executar
- Inicie o Zookeeper e o Kafka;
- Entre na pasta do projeto;
- Rode o comando:

  ```bash
  python3 producer_consumer_mongodb.py
  ```

### [Python Kafka](/python-kafka)
- 

## Bibliotecas üìö
* [ Kafka ](https://kafka.apache.org/documentation/)
* [ Spark ](https://spark.apache.org/docs/latest/)
* [ Python ](https://docs.python.org/pt-br/3/)

----------------------------
## Colaboradores üß§
[ Roberto Bezerra Morel Lopes ](https://www.linkedin.com/in/roberto-morel-6b9065193/)

[ Lucas Martins Belmino ](https://www.linkedin.com/in/)

[ Venicius Gomes Santiago ](https://www.linkedin.com/)